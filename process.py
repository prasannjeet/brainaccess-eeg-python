import io
import numpy as np
import matplotlib.pyplot as plt
import mne
from scipy.io import wavfile
from analyse import fetch_first_user_fif_blob, fetch_first_user_audio
from utilities import preprocess_raw_data, add_plot_title

# Fetch the FIF blob of the first user
fif_blob = fetch_first_user_fif_blob()

# Create a BytesIO object from the blob
fif_buffer = io.BytesIO(fif_blob)

# Read the FIF data from the buffer
raw = mne.io.read_raw_fif(fif_buffer, preload=True)

# Preprocess the raw data
raw = preprocess_raw_data(raw)

fig2 = raw.plot(scalings='auto', verbose=False)
plt.show()

# We save the scaling generated by auto-scaling the plot, and use it further for cropped data
scaling_after_auto = fig2.mne.scalings

# Extract the annotations
annotations = raw.annotations

# Find the Q and R annotations and their timestamps
q_r_pairs = []
for i, description in enumerate(annotations.description):
    if description.startswith('Q'):
        q_time = annotations.onset[i]  # Get the onset time of the Q annotation
        r_description = 'R' + description[1:]
        r_times = annotations.onset[annotations.description == r_description]
        if len(r_times) > 0:
            r_time = r_times[0]  # Get the onset time of the corresponding R annotation
            q_r_pairs.append((q_time, r_time))  # Store the onset times as a pair
            print(f"Found a pair: {description} & {r_description}")

# Print the number of pairs found
print(
    f"Found {len(q_r_pairs)} pairs. Dividing the plot into {len(q_r_pairs)} parts and displaying each plot separately.")

raw_segment_array = []

# Plot the segments between Q and R annotations
for i, (q_time, r_time) in enumerate(q_r_pairs):
    print(f"Plotting segment {i}: {q_time} to {r_time}")
    # Crop the data to the desired segment using the onset times of the Q and R annotations
    raw_segment = raw.copy().crop(tmin=q_time, tmax=r_time).load_data()
    raw_segment_array.append(raw_segment)

    events, event_id = mne.events_from_annotations(raw_segment)
    fig = raw_segment.plot(scalings=scaling_after_auto, verbose=False, title=f"Segment {i}: {q_time} to {r_time}")
    add_plot_title(f"Graph for Question {i + 1}", fig)

    plt.show()

# raw_segment_array = []
#
# # Plot the segments between Q and R annotations
# for i, (q_time, r_time) in enumerate(q_r_pairs):
#     print(f"Plotting segment {i}: {q_time} to {r_time}")
#     # Crop the data to the desired segment using the onset times of the Q and R annotations
#     raw_segment = raw.copy().crop(tmin=q_time, tmax=r_time).load_data()
#     raw_segment_array.append(raw_segment)
#
#     events, event_id = mne.events_from_annotations(raw_segment)
#     raw_segment.plot(scalings=scaling_after_auto, verbose=False, title=f"Segment {i}: {q_time} to {r_time}")
#     plt.pause(0.1)  # Add a short pause to allow the plot to render
#
# # Keep all the plots open
# plt.show(block=True)

audio_blobs = fetch_first_user_audio()
raw_segment_array = []
data_array = []

# Extract data from each segment
for i, (q_time, r_time) in enumerate(q_r_pairs):
    raw_segment = raw.copy().crop(tmin=q_time, tmax=r_time).load_data()
    raw_segment_array.append(raw_segment)

    # Extract data and times for the segment
    data, times = raw_segment[:, :]
    data_array.append((data, times))

# Plot the segments using Matplotlib
fig, axs = plt.subplots(len(q_r_pairs), 1, figsize=(10, 4 * len(q_r_pairs)))
for i, (data, times) in enumerate(data_array):
    for j, channel_data in enumerate(data):
        axs[i].plot(times, channel_data, label=raw.ch_names[j])
    axs[i].set_title(f"Segment {i}: {q_r_pairs[i][0]} to {q_r_pairs[i][1]}")
    axs[i].legend(loc="upper right")

plt.tight_layout()
plt.show()

